'use client';

import React, { useState, useEffect, useRef } from 'react';
import { useVitality } from '../context/VitalityContext';
import Icon from './Icon';

/**
 * [Phase 3.0] VoiceAssistant (ë§ˆì‹¤ì´ 2.0)
 * ì‹¤ì‹œê°„ ìŒì„± ìƒí˜¸ì‘ìš© ë° ëŠ¥ë™ì  ê±´ê°• ì½”ì¹­ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
 */
export default function VoiceAssistant() {
  const { steps, triggerVoiceCoach, user } = useVitality();
  const [isActive, setIsActive] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const recognitionRef = useRef(null);

  // Web Speech API ì´ˆê¸°í™”
  useEffect(() => {
    if (typeof window !== 'undefined' && (window.SpeechRecognition || window.webkitSpeechRecognition)) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = false;
      recognitionRef.current.lang = 'ko-KR';

      recognitionRef.current.onresult = (event) => {
        const text = event.results[0][0].transcript;
        setTranscript(text);
        processCommand(text);
        setIsListening(false);
      };

      recognitionRef.current.onerror = () => setIsListening(false);
      recognitionRef.current.onend = () => setIsListening(false);
    }
  }, []);

  // [Phase 3.0] Active Coaching Logic
  useEffect(() => {
    if (!user) return;
    
    // ì˜ˆ: ëª©í‘œì˜ 50% ë‹¬ì„± ì‹œ ëŠ¥ë™ì  ì¹­ì°¬
    if (steps > 0 && steps === 2000) {
      const utterance = new SpeechSynthesisUtterance("ë²Œì¨ 2ì²œ ë³´ë‚˜ ê±¸ìœ¼ì…¨ë„¤ìš”! ëŒ€ë‹¨í•˜ì„¸ìš” ì–´ë¥´ì‹ . ì¡°ê¸ˆë§Œ ë” í˜ë‚´ì„¸ìš”!");
      utterance.lang = 'ko-KR';
      window.speechSynthesis.speak(utterance);
      setTranscript("í™œê¸°ì°¬ ê±¸ìŒ, ì‘ì›í•©ë‹ˆë‹¤! ğŸ‰");
    }
  }, [steps, user]);

  const startListening = () => {
    if (recognitionRef.current) {
      setTranscript('ë“£ê³  ìˆì–´ìš”... ğŸ‘‚');
      setIsListening(true);
      recognitionRef.current.start();
    } else {
      alert('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
    }
  };

  const processCommand = (text) => {
    console.log('ğŸ—£ï¸ User said:', text);
    if (text.includes('ê±¸ìŒ') || text.includes('ì–¼ë§ˆë‚˜')) {
      triggerVoiceCoach('steps');
    } else if (text.includes('ì•ˆë…•') || text.includes('ë§ˆì‹¤')) {
      const utterance = new SpeechSynthesisUtterance("ì•ˆë…•í•˜ì„¸ìš” ì–´ë¥´ì‹ ! ë§ˆì‹¤ì´ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì»¨ë””ì…˜ì€ ì–´ë– ì‹ ê°€ìš”?");
      utterance.lang = 'ko-KR';
      window.speechSynthesis.speak(utterance);
    } else {
      const utterance = new SpeechSynthesisUtterance(`${text}ë¼ê³  ë§ì”€í•˜ì…¨êµ°ìš”. ì•„ì§ í•™ìŠµ ì¤‘ì´ë¼ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”.`);
      utterance.lang = 'ko-KR';
      window.speechSynthesis.speak(utterance);
    }
  };

  if (!user) return null;

  return (
    <div className="fixed bottom-24 right-6 z-50">
      {/* í…ìŠ¤íŠ¸ ë§í’ì„  */}
      {isListening || transcript ? (
        <div className="bg-white p-3 rounded-2xl shadow-xl mb-3 max-w-[200px] border border-blue-100 animate-bounce">
          <p className="text-sm text-gray-700 font-medium">{transcript || 'ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'}</p>
        </div>
      ) : null}

      {/* ë§ˆì‹¤ì´ ë²„íŠ¼ */}
      <button
        onClick={startListening}
        className={`p-4 rounded-full shadow-2xl transition-all active:scale-90 ${
          isListening ? 'bg-red-500 animate-pulse' : 'bg-blue-600 hover:bg-blue-700'
        }`}
      >
        <div className="text-white relative">
            <Icon name="Mic" size={32} />
            {isListening && (
                <div className="absolute -top-1 -right-1 w-3 h-3 bg-white rounded-full animate-ping" />
            )}
        </div>
      </button>

      <style jsx>{`
        .bg-white { transform-origin: bottom right; }
      `}</style>
    </div>
  );
}
